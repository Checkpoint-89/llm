{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\cdiet\\miniconda3\\envs\\hf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error() #debug, info, warning, error, critical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tasks = [\"feature-extraction\", \"fill-mask\", \"ner\", \"question-answering\", \n",
    " \"sentiment-analysis\", \"summarization\", \"text-generation\", \n",
    " \"translation\", \"zero-shot-classification\", ]\n",
    "\n",
    "task = tasks[4]\n",
    "\n",
    "pline = pipeline(task)\n",
    "pline(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behind the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Inputs (after tokenization):  {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "- Transformer - size of last hidden state:  torch.Size([2, 16, 768])\n",
      "\n",
      "- Head - size of logits:  torch.Size([2, 2])\n",
      "\n",
      "- Predictions:  tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "- id2label:  {0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "model_w_head = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "out_wo_head = model(**inputs)\n",
    "out_w_head = model_w_head(**inputs)\n",
    "predictions = torch.nn.functional.softmax(out_w_head.logits, dim=-1)\n",
    "\n",
    "print(\"\\n- Inputs (after tokenization): \", inputs)\n",
    "print(\"\\n- Transformer - size of last hidden state: \", out_wo_head.last_hidden_state.shape)\n",
    "print(\"\\n- Head - size of logits: \", out_w_head.logits.shape)\n",
    "print(\"\\n- Predictions: \", predictions)\n",
    "print(\"\\n- id2label: \", model_w_head.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Inputs <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "- Model <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n",
      "- Out_wo_head <class 'transformers.modeling_outputs.BaseModelOutput'>\n",
      "- Model_w_head <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n",
      "- Out_w_head <class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n",
      "- Predictions <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print('- Inputs', type(inputs))\n",
    "print('- Model', type(model))\n",
    "print('- Out_wo_head', type(out_wo_head))\n",
    "print('- Model_w_head', type(model_w_head))\n",
    "print('- Out_w_head', type(out_w_head))\n",
    "print('- Predictions', type(predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb hidden layers: 12\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config with a specific Config class\n",
    "config = BertConfig()\n",
    "\n",
    "# Initializing a specific Model class with random weights\n",
    "model = BertModel(config)\n",
    "\n",
    "# Initializing the model with pre-trained weights\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\".\\models\")\n",
    "\n",
    "print(f'Nb hidden layers: {model.config.num_hidden_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relaod the model with the first method: \n",
      "Nb hidden layers: 10\n",
      "\n",
      "Relaod the model with the second method: \n",
      "Nb hidden layers: 10\n"
     ]
    }
   ],
   "source": [
    "# Reload the model while adapting a parameter from the config\n",
    "\n",
    "# First method to reload the model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('.\\models', num_hidden_layers=10, )\n",
    "print(\"\\nRelaod the model with the first method: \")\n",
    "print(f'Nb hidden layers: {model.config.num_hidden_layers}')\n",
    "\n",
    "# Second method to reload the model\n",
    "from transformers import AutoConfig, AutoModel\n",
    "config = AutoConfig.from_pretrained('.\\models', num_hidden_layers=10)\n",
    "model = AutoModel.from_pretrained('.\\models', config=config)\n",
    "print(\"\\nRelaod the model with the second method: \")\n",
    "print(f'Nb hidden layers: {model.config.num_hidden_layers}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization the usual way:\n",
      "- Tokens:  {'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "- Decode input IDs:  [CLS] Using a Transformer network is simple [SEP]\n",
      "\n",
      "Tokenization step by step:\n",
      "- Tokens:  ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "- IDs:  [7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "- Decode input IDs:  Using a Transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "\n",
    "# The usual way to use a tokenizer\n",
    "tokens1 = tokenizer(sequence)\n",
    "print(\"\\nTokenization the usual way:\")\n",
    "print(\"- Tokens: \", tokens1)\n",
    "print(\"- Decode input IDs: \", tokenizer.decode(tokens1['input_ids']))\n",
    "\n",
    "# Step by step\n",
    "tokens2 = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "print(\"\\nTokenization step by step:\")\n",
    "print(\"- Tokens: \", tokens2)\n",
    "print(\"- IDs: \", ids)\n",
    "print(\"- Decode input IDs: \", tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching Sequences: why we need attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seq1:  [[200, 200, 200]]\n",
      "Logits seq1:    tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Seq2:  [[200, 200]]\n",
      "Logits seq2:    tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Batched:  [[200, 200, 200], [200, 200, 0]]\n",
      "Logit batched:  tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "The logits of the batched sequence are different from the logits of the second shorter sequence\n",
      "This is because the second sequence of the batch has been padded. That padding was used by the attention layers; this was not the case with the second sequence.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Let's imagine, the tokenization results in the following input ids:\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "# Let's compute the output for each input\n",
    "logits_seq1 = model(torch.tensor(sequence1_ids)).logits\n",
    "logits_seq2 = model(torch.tensor(sequence2_ids)).logits\n",
    "logit_batched = model(torch.tensor(batched_ids)).logits\n",
    "\n",
    "print(\"\\nSeq1: \", sequence1_ids)\n",
    "print(\"Logits seq1:   \", logits_seq1)\n",
    "\n",
    "print(\"\\nSeq2: \", sequence2_ids)\n",
    "print(\"Logits seq2:   \", logits_seq2)\n",
    "\n",
    "print(\"\\nBatched: \", batched_ids)\n",
    "print(\"Logit batched: \", logit_batched)\n",
    "\n",
    "# There is a difference betweeen the two approaches for sequence 2, what happened ?\n",
    "print(\"\\nThe logits of the batched sequence are different from the logits of the second shorter sequence\")\n",
    "print(\"This is because the second sequence of the batch has been padded. That padding was used by the attention layers; this was not the case with the second sequence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence batching: use of attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Correct the difference with attention_mask \n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From tokenizer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One step of Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\cdiet\\miniconda3\\envs\\hf\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4374, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "pred = model(**batch)\n",
    "loss = pred.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mglue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmrpc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m raw_datasets\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
