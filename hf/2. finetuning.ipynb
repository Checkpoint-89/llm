{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
    "\n",
    "import datasets\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(category=FutureWarning ,action='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple training in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 181.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "raw_dataset['train'].features.keys():  dict_keys(['sentence1', 'sentence2', 'label', 'idx'])\n",
      "raw_dataset['train'].num_rows:  3668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tokenized_datasets['train'].features.keys():  dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tokenized_datasets['train'].num_rows:  3668\n",
      "\n",
      "Note that the batched=True argument does not result in a batched output\n",
      "raw_dataset['train] and tokenized_datasets['train'] have the same number of rows.\n",
      "=> batched=True instructs the tokenizer to process per batch and so speed up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Define the checkpoint\n",
    "checkpoint = 'bert-base-uncased'\n",
    "\n",
    "# Import the DatasetDict\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "print(\"\\nraw_dataset['train'].features.keys(): \", raw_dataset['train'].features.keys())\n",
    "print(\"raw_dataset['train'].num_rows: \", raw_dataset['train'].num_rows)\n",
    "\n",
    "# Tokenize the Datasets in DatasetDict\n",
    "# The tokenizer returns a dictionnary. We want to keep the DatasetDict type.\n",
    "# So we use the .map() method with with_indices=True and batched=True\n",
    "from transformers import AutoTokenizer\n",
    "def tokenize_function(sequences):\n",
    "    return tokenizer(sequences['sentence1'], sequences['sentence2'], truncation=True)\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function , batched=True)\n",
    "print(\"\\tokenized_datasets['train'].features.keys(): \", tokenized_datasets['train'].features.keys())\n",
    "print(\"tokenized_datasets['train'].num_rows: \", tokenized_datasets['train'].num_rows)\n",
    "\n",
    "print(\"\\nNote that the batched=True argument does not result in a batched output\")\n",
    "print(\"raw_dataset['train] and tokenized_datasets['train'] have the same number of rows.\")\n",
    "print(\"=> batched=True instructs the tokenizer to process per batch and so speed up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching the DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len of samples['input_ids']:  [50, 59, 47, 67, 59, 50, 62, 32]\n",
      "\n",
      "len of dyn_padded_batch['input_ids']:  [67, 67, 67, 67, 67, 67, 67, 67]\n"
     ]
    }
   ],
   "source": [
    "# Define the data collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Take a batch of data, keep only the relevant information for training\n",
    "batch_samples = tokenized_datasets[\"train\"][:8]\n",
    "batch_samples = {k: v for k, v in batch_samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "print(\"\\nlen of samples['input_ids']: \", [len(x) for x in batch_samples[\"input_ids\"]])\n",
    "\n",
    "# Pad the batch\n",
    "dyn_padded_batch = data_collator(batch_samples)\n",
    "print(\"\\nlen of dyn_padded_batch['input_ids']: \", [len(x) for x in dyn_padded_batch[\"input_ids\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5231, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3052, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n",
      "{'train_runtime': 179.3616, 'train_samples_per_second': 61.351, 'train_steps_per_second': 7.677, 'train_loss': 0.3422743077717922, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3422743077717922, metrics={'train_runtime': 179.3616, 'train_samples_per_second': 61.351, 'train_steps_per_second': 7.677, 'train_loss': 0.3422743077717922, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TrainingArguments and Trainer\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "# Define the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Define the Trainer\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Quick training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8627450980392157, 'f1': 0.9044368600682594}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the trainer to do predictions\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "# Convert logits into integers\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Evaluate the predictions\n",
    "import evaluate\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46431678533554077, 'eval_accuracy': 0.8357843137254902, 'eval_f1': 0.8870151770657673, 'eval_runtime': 2.4582, 'eval_samples_per_second': 165.978, 'eval_steps_per_second': 20.747, 'epoch': 1.0}\n",
      "{'loss': 0.5664, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n",
      "{'eval_loss': 0.46998754143714905, 'eval_accuracy': 0.8284313725490197, 'eval_f1': 0.8852459016393442, 'eval_runtime': 2.4604, 'eval_samples_per_second': 165.826, 'eval_steps_per_second': 20.728, 'epoch': 2.0}\n",
      "{'loss': 0.3747, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n",
      "{'eval_loss': 0.5406590104103088, 'eval_accuracy': 0.8651960784313726, 'eval_f1': 0.9056603773584906, 'eval_runtime': 2.789, 'eval_samples_per_second': 146.29, 'eval_steps_per_second': 18.286, 'epoch': 3.0}\n",
      "{'train_runtime': 187.5065, 'train_samples_per_second': 58.686, 'train_steps_per_second': 7.344, 'train_loss': 0.4061395981732537, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.4061395981732537, metrics={'train_runtime': 187.5065, 'train_samples_per_second': 58.686, 'train_steps_per_second': 7.344, 'train_loss': 0.4061395981732537, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function that compute metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Redefine the Trainer\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Retrain the model and get metric after each epoch/ step\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
