{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from importlib import reload\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import OPTForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import evaluate\n",
    "\n",
    "import utils.preprocess_data\n",
    "reload(utils.preprocess_data)\n",
    "from utils.preprocess_data import preprocess_orig_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set-up completed\n"
     ]
    }
   ],
   "source": [
    "# Set up DL framework and device\n",
    "dl_framework = 'pt'\n",
    "\n",
    "is_gpu = torch.cuda.is_available()\n",
    "if is_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Set up file and dir paths\n",
    "clean = True # Whether to clean the raw data directory\n",
    "data_type = 'applications' # 'is_experimental' or 'applications'\n",
    "\n",
    "path_to_galactica_folder = Path(r'../galactica')\n",
    "\n",
    "if data_type == 'applications':\n",
    "    path_to_raw = Path(r'./data/raw_applications.json')\n",
    "elif data_type == 'is_experimental':\n",
    "    path_to_raw = Path(r'./data/raw_is_experimental.json')\n",
    "path_to_data = Path(path_to_galactica_folder, path_to_raw)\n",
    "\n",
    "path_to_state_dict = \"./state_dict/model_state_dict.pt\"\n",
    "path_to_state_dict = Path(path_to_state_dict)\n",
    "dir_to_state_dict = path_to_state_dict.parent\n",
    "shutil.rmtree(dir_to_state_dict, ignore_errors=True)\n",
    "os.mkdir(dir_to_state_dict)\n",
    "\n",
    "# Set up data\n",
    "preprocess = False\n",
    "subset_size = 128\n",
    "\n",
    "# Set up model instanciation\n",
    "checkpoint = \"facebook/galactica-125m\"\n",
    "num_hidden_layers = 1\n",
    "print(\"\\nSet-up completed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (D:/Users/cdiet/hf/datasets/json/default-ead8f05166997550/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.67it/s]\n",
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to get raw data into /galactica/data\n",
    "if preprocess == True:\n",
    "    if data_type == 'applications':\n",
    "        path_to_orig = Path(r'./data/orig_applications.json')\n",
    "    elif data_type == 'is_experimental':\n",
    "        path_to_orig = Path(r'./data/orig_is_experimental.json')\n",
    "\n",
    "    preprocess_orig_data(str(path_to_galactica_folder), str(path_to_orig), clean=clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data, model, tokenizer and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels'],\n",
      "        num_rows: 96044\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels'],\n",
      "        num_rows: 12005\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels'],\n",
      "        num_rows: 12006\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of labels:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/galactica-125m were not used when initializing OPTForSequenceClassification: ['model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.fc1.bias', 'lm_head.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.fc1.weight']\n",
      "- This IS expected if you are initializing OPTForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/galactica-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated\n",
      "\n",
      "Parameters frozen\n",
      "\n",
      "Max length =  768\n",
      "\n",
      "Tokenizer instantiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 128\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 128\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_labels', 'id', 'title', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 128\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "raw_dataset = load_from_disk(str(path_to_data))\n",
    "print(\"\\nDataset loaded: \", raw_dataset)\n",
    "\n",
    "# Take a subset of raw_dataset\n",
    "# TODO: implement the following method, much more elegant\n",
    "# TODO: small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "ds_dict = dict()\n",
    "for split in raw_dataset:\n",
    "    ds = raw_dataset[split]\n",
    "    ds = Dataset.from_dict(ds[0:subset_size])\n",
    "    ds = ds.cast(raw_dataset[split].features)\n",
    "    ds_dict[split] = ds\n",
    "raw_dataset=DatasetDict(ds_dict)\n",
    "\n",
    "# Get the number of labels\n",
    "num_labels = len(raw_dataset['train'].features['labels'].names)\n",
    "print(\"\\nNumber of labels: \", num_labels)\n",
    "\n",
    "# Load the Model\n",
    "#TODO: understand how to properly instantiate the model when device.cuda == 'cuda'\n",
    "model = OPTForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels, num_hidden_layers=num_hidden_layers)\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, str(path_to_state_dict))\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    config = AutoConfig.from_pretrained(checkpoint, num_labels=num_labels, num_hidden_layers=num_hidden_layers)\n",
    "    with init_empty_weights():\n",
    "        model = OPTForSequenceClassification._from_config(config)\n",
    "    model.tie_weights()\n",
    "    no_split_module_classes = None #List of modules with any residual connection of some kind\n",
    "    model = load_checkpoint_and_dispatch(model, str(path_to_state_dict), device_map=\"auto\", no_split_module_classes=no_split_module_classes)\n",
    "print(\"\\nModel instantiated\")\n",
    "\n",
    "# Freeze the model but the last layer\n",
    "# TODO: how to programmatically get the name of the last layer\n",
    "for param in model.named_parameters():\n",
    "    if param[0] != 'score.weight':\n",
    "        param[1].requires_grad = False\n",
    "print(\"\\nParameters frozen\")\n",
    "\n",
    "# TODO: This is probably wrong, check what is the correct max_length\n",
    "max_length = model.config.word_embed_proj_dim\n",
    "print(\"\\nMax length = \", max_length)\n",
    "\n",
    "# Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True) # use_fast is recommended to be False, set to True for testing purposes\n",
    "# use_fast argument; check https://huggingface.co/docs/transformers/model_doc/opt#overview\n",
    "id2label = {i: label for label, i in tokenizer.vocab.items()}\n",
    "pad_token_id = model.config.pad_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': id2label[pad_token_id]})\n",
    "print(\"\\nTokenizer instantiated\")\n",
    "\n",
    "def tokenize_function(sequences):\n",
    "    return tokenizer(sequences['text'], max_length=max_length, truncation=True)\n",
    "\n",
    "#TODO: save the tokenized datasets to disk\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function , batched=True)\n",
    "print(\"\\nTokenized datasets: \", tokenized_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 1.40MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3333333333333333}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pred = ([[0.5, 0.2, 0.6], [0.2, 0.9, 0.2], [0.1,0.0,0.0]], [1,1,2])\n",
    "logits, labels = eval_pred\n",
    "preds = np.argmax(logits, axis = -1)\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "accuracy_metric.compute(predictions=preds, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2] [1, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_pred,\n",
    "                    accuracy_metric = evaluate.load('accuracy'),\n",
    "                    f1_metric = evaluate.load('f1'),\n",
    "                    precision_metric = evaluate.load('precision'),\n",
    "                    recall_metric = evaluate.load('recall')):\n",
    "\n",
    "    results = {}\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis = -1)\n",
    "    print(preds, labels)\n",
    "    results.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=preds, references = labels, average=\"micro\"))\n",
    "    results.update(precision_metric.compute(predictions=preds, references = labels, average=\"micro\"))\n",
    "    results.update(recall_metric.compute(predictions=preds, references = labels, average=\"micro\"))\n",
    "    return results\n",
    "\n",
    "eval_pred = ([[0.5, 0.7, 0.6], [0.2, 0.9, 0.2], [0.1,0.1,0.9]], [1,1,2])\n",
    "compute_metrics(eval_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataCollator instantiated\n",
      "\n",
      "TrainingArguments instantiated\n",
      "\n",
      "Cleaning done\n",
      "\n",
      "Trainer instantiated\n"
     ]
    }
   ],
   "source": [
    "# Define the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "print(\"\\nDataCollator instantiated\")\n",
    "\n",
    "# Define the TrainingArguments and Trainer\n",
    "# Default parameters in comments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"test-trainer\",\n",
    "    overwrite_output_dir = False, # False\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    num_train_epochs = 3, # 3\n",
    "    per_device_train_batch_size = 64, # 8\n",
    "    per_device_eval_batch_size = 64, # 8\n",
    "    gradient_accumulation_steps = 1, # 1\n",
    "    learning_rate = 5e-5, # 5e-5\n",
    "    weight_decay = 0, # 0\n",
    "    warmup_steps = 0, # 0\n",
    "    evaluation_strategy = 'epoch', # 'no' by default, others: 'epoch', 'steps'\n",
    "    )\n",
    "print(\"\\nTrainingArguments instantiated\")\n",
    "\n",
    "# Cleaning\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nCleaning done\")\n",
    "\n",
    "# Define the evaluation metrics\n",
    "def compute_metrics(eval_pred, average = 'micro', # average in [None, 'micro', 'macro', 'weighted']\n",
    "                    accuracy_metric = evaluate.load('accuracy'),\n",
    "                    f1_metric = evaluate.load('f1'),\n",
    "                    precision_metric = evaluate.load('precision'),\n",
    "                    recall_metric = evaluate.load('recall')):\n",
    "\n",
    "    results = {}\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis = -1)\n",
    "    print(preds, labels)\n",
    "    results.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=preds, references = labels, average=average))\n",
    "    results.update(precision_metric.compute(predictions=preds, references = labels, average=average))\n",
    "    results.update(recall_metric.compute(predictions=preds, references = labels, average=average))\n",
    "    return results \n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"\\nTrainer instantiated\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      2\u001b[0m output\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2698\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2699\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2702\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1058\u001b[0m, in \u001b[0;36mOPTForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1058\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1059\u001b[0m     input_ids,\n\u001b[0;32m   1060\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1061\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1062\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1063\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1064\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1065\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1066\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1067\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1068\u001b[0m )\n\u001b[0;32m   1069\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1070\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:789\u001b[0m, in \u001b[0;36mOPTModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    788\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m    790\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    792\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    793\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    794\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    795\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    796\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    797\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    798\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    799\u001b[0m )\n\u001b[0;32m    801\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m    802\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:704\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    696\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    697\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    698\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    702\u001b[0m     )\n\u001b[0;32m    703\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    705\u001b[0m         hidden_states,\n\u001b[0;32m    706\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[0;32m    707\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    708\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    709\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    710\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    711\u001b[0m     )\n\u001b[0;32m    713\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:329\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[0;32m    326\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    328\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    330\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    331\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    332\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    333\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    334\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    335\u001b[0m )\n\u001b[0;32m    336\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    337\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:192\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    189\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[39m# self_attention\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m    193\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m    196\u001b[0m     \u001b[39m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[39m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[39m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\cdiet\\miniconda3\\envs\\galactica\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galactica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
